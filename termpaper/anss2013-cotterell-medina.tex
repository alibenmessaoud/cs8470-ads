% File: anss2013-cotterell-medina.tex
% Author: Terrance Medina and Michael Cotterell
%
\documentclass[letterpaper,twocolumn,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{scs}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{epsfig,epstopdf}
\usepackage{hyperref}
\usepackage{tkz-graph}
%\usepackage{citesort}

\usetikzlibrary{arrows}

\renewcommand\thesection{\arabic{section}}

\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.35}

\begin{document}

\newtheorem{defn}{Definition}

\title{A Markov Model for Ontology Alignment}

\author{Michael E. Cotterell and Terrance Medina \\
Department of Computer Science \\
The University of Georgia \\
Athens, GA 30602-7404 \\
mepcott@uga.edu, medinat@uga.edu
}

\maketitle

\begin{abstract}
Ontology Alignment -- problem importance

Need for a way to determine convergence.

How does a Markov chain perform analytically?

\end{abstract}


\section{Introduction}

\begin{itemize}
\item Normalized Levenshtein Disance~\cite{yujian:2007:levenshtein}. 
\item Similarity Flooding~\cite{melnik:2002:similarity}.
\item Probability and Computing book~\cite{mitzenmacher:2005:probability}.
\item Information-Theoretic~\cite{lin:1998:information}.
\item Hungarian algorithm~\cite{kuhn:1955:hungarian}.
\item Ontology Matching book~\cite{euzenat:2007:ontology}.
\item ScalaTion~\cite{miller:2010:scalation}.
\item BCNN book~\cite{bcnn:2010:simulation}.
\item Scala spec~\cite{odersky:2011:spec}.
\item My OEI paper~\cite{cotterell:2012:oei}.
\item Linear Algebra textbook~\cite{goodaire:2003:linalgebra}.
\end{itemize}

An ontology alignment may be informally described as a set of correspondences between semantically related terms in two heterogeneous input ontologies. 
Each correspondence is qualified with a confidence level, $[0,1]$.
Alignment is useful in data integration tasks dealing with what is sometimes referred to as the semantic heterogeneity problem. 
It helps in the automation of various important tasks, most important of which is schema merging, enabling the knowledge and data expressed in the input ontologies to interoperate. \\

\noindent The rest of this paper is organized as follows: the remainder of this section will cover some of the background material related to the Semantic Web and ontologies, the cases for fully automated alignment, and some of the background material on the statistical methods used in this paper; Section~\ref{sec:approach} outlines the approach and implementation details; the evaluation and benchmarks are explained in Section~\ref{sec:eval}; results are presented in Section~\ref{sec:results}; and, conclusions and possible future work are outlined in Section~\ref{sec:conclusions}.

\subsection{Semantic Web and Ontologies}
\label{subsec:semanticweb}

%Knowledge bases
A knowledge base is a kind of database that is designed 
for decision support systems and expert systems. It specifically allows
machines to perform deductive reasoning over its elements.
For example, given the instance data ``John is a farmer'' and ``Farmers
wear overalls'', a knowledge base reasoner could deduce the new information
that ``John wears overalls''.

%KB representation
Knowledge bases (KBs) consist of entities and
relations between those entites. They are often represented as sets of
triples, that is, a set of two entities and a relationship between those
entities. For example, the triple ${\{teachers, lesson\_plans, write\}}$
defines the relationship that teachers write lesson\_plans.

%Ontologies
Ontologies describe the relationships between elements in a knowledge base.
Similar to the notion of schemas in relational databases, ontologies specify
the structural relationships amongst the entities and classes of the ontology.

%Heterogeneous Data Problem
Frequently, queries must be performed across multiple knowledge bases. This is the
case when departments in a large enterprise maintain their own knowledge bases,
but must also share information with other departments. It is also a fundamental
requirement of the semantic web; users perform queries against the web,
and those queries must be performed against knowledge bases that may belong to 
entities on opposite sides of the globe. This leads us to the problem of heterogeneous
data. When knowledge bases are maintained by different organizations, the ontologies
used to describe them will usually be different. They will use different names
to describe similar or identical concepts, for instance, one ontology may keep track
of `cars' while another keeps track of `automobiles' and still another keeps track of 
`autoProducts'. In fact the list could go on and on.

%Ontology Alignnment
Ontology Alignment is the process of equating two heterogeneous ontologies by
finding valid correspondences between their sets of elements. 
%
For instance, an alignment between an auto parts manufacturer and an auto dealership
might tell us with 85\% certainty that ``anti\_lock\_brakes'' and ``ABS'' represent
the same thing.

%Importance of Ontology Alignment
This is important because it allows a query management system to translate the terminology
of a user's query into the terminology used by many different ontologies and thereby
to query heterogeneous knowledge bases.

%Contribution
We present a novel representation for ontology alignment that allows for improved
convergence analysis.

\subsection{Why do we need fully automated alignment?}
\label{subsec:automated}

TODO

\subsection{Statistical \& Simulation Background}
\label{subsec:stat}

%Markov Processes
A Markov Process is a mathematical model that represents a system as a set of states
and a set of probabilistic transitions between those states. Most importantly, Markov 
processes adhere to the Markov Property, that is, a system's next state depends only
on its current state, and not on any of its previous states. In that sense, Markov
processes are called ``memoryless''.

%Hidden Markov Models
A Hidden Markov Model (HMM) is simply a Markov process whose states and transitions
are unobservable.

\section{Related Work}

Although the problem of fully-automated ontology alignment is far from being solved,
there has been much work accomplished around fundamental techniques for element
matching and aggregation of those matching techniques. One class of approaches
attempts to find element matches based on the relative similarity or dissimilarity
of the actual labels giving to those elements. Some examples of these string-matching 
techniques are the Jaro-Winkler measure, the Levenshtein Distance and latent semantic 
indexing.
%
While useful in many situations, string-based matching techniques suffer from a common 
shortcoming; similar real-world objects often have very dissimilar names. The words
``car'' and ``autombile'' provide a good example.

Another class of techniques makes use of outside resources as aids in the search for
good matches between elements. Such outside resources include dictionaries or taxonomies
such as the WordNet taxonomy. These classes of techniques attempt to produce correspondences
between elements that may likely refer to the same real-world objects, but that have very
dissimilar names, such as ``car'' and ``automobile''. Some examples of these semantic 
approaches include Information-theoretic similarity.
%
The use of outside taxonomies can greatly improve the quality of matching results,
but such outside resources are not always readily available.

Yet a third class of matching techniques attempts to exploit the structure of the ontology
itself. For example, Similarity Flooding takes a directed graph representation of an 
ontology and uses neighbor relations between the elements to find matches correspondences
between them. The idea is that if two elements in heterogeneous ontologies are very similar,
then their neighboring elements should also be very similar.
%
%Convergence Properties
Similarity flooding has been useful as a foundation is several other structure-based
matching techniques such as anchor flooding \textbf{**others??*} but suffers from 
a few limitations. First, it requires that the edges of the edge-labeled graph 
representation have identically named labels. It can also be difficult to predict
wether or not the fixpoint calculation will converge.
\newline

\section{Approach \& Implementation}
\label{sec:approach}

The following sections outline and present the details about the model as well as many of the implementation details.

\subsection{Levenshtein Edit Distance}

\begin{defn}
Let $x$ and $y$ be terms in a single ontology. The label set $\Lambda \left( x, y \right)$ is the set of all labels between hierarchical properties and object properties where $x$ and $y$ are the domain and range of a property, respectively.
\end{defn}

\begin{figure*}
\centering
\begin{equation*}
\mathcal{L} 
\left( \alpha_i, \beta_j \right) = \left\{
	\begin{array}{ll}
   	 	0 &: i=j=0 \\
		i &: j = 0 \text{ and } i > 0 \\
		j &: i = 0 \text{ and } j > 0 \\
		\min 
			\left\{ 
			\begin{array}{l}
				\mathcal{L} \left( \alpha_{i-1}, \beta_j \right) + 1 \\
          		        \mathcal{L} \left( \alpha_i, \beta_{j-1} \right) + 1 \\
          		        \mathcal{L} \left( \alpha_{i-1}, \beta_{j-1} \right) + [\alpha_i \neq \beta_j]
			\end{array} \right. &: \text{else}
     \end{array}
\right.
\end{equation*}
\caption{Levenshtein Edit Distance}
\end{figure*}

\subsection{Edge Confidence}

\begin{defn}
Let $\gamma$ be some threshold. Edge confidence $\Gamma \left( \alpha, \beta \right)$ is the similarity score between the labels of two edges $\alpha$ and $\beta$ if that similarity is greater than $\gamma$, otherwise $0$. That is,
$$ \Gamma \left( \alpha, \beta \right) = \left\{
   	 \begin{array}{ll}
           \frac{1}{\sigma \left( \alpha, \beta \right)} & : \vert \sigma \left( \alpha, \beta \right) \vert \geq \gamma \\
           0                                             & : \mathrm{otherwise}
     \end{array}
   \right.$$
\end{defn}

\subsection{Unnormalized Pairwise Markov Chain}

Consider the input ontologies presented in Figure~\ref{fig:input}.

\begin{defn}
An {\bf Unnormalized Pairwise Markov Chain} is a not-necessarily stochastic Markov Chain that satisfies the following property.
For every pairwise grouping of ontological terms between the two input ontologies, there exists a transition in the UPMC
$$ (x, y) \rightarrow (x', y') $$
with probability $\Gamma(\Lambda(x, y), \Lambda(x', y'))$ if and only if
\begin{itemize}
\item there exists an edge from $x$ to $x'$, and
\item there exists an edge from $y$ to $y'$.
\end{itemize}
\end{defn}

\begin{figure*}
\centering
\SetVertexNormal[
	Shape = circle,
    % FillColor = orange,
    LineWidth = 1pt
]
\SetUpEdge[
	lw = 1pt,
    color = orange,
    labelcolor = white
]
\begin{tikzpicture}
   \Vertex[x=0, y=0]{A}
   \Vertex[x=2, y=2]{B}
   \Vertex[x=4, y=0]{C}
   
   \Vertex[x=6, y=0]{D}
   \Vertex[x=8, y=2]{E}
   \Vertex[x=10, y=0]{F}
%   \tikzset{EdgeStyle/.append style = {bend left}}
   \tikzset{EdgeStyle/.style={->}}
   \Edge[label = $m$](A)(B)
   \Edge[label = $n$](B)(C)
   \Edge[label = $o$](C)(A)
   
   \Edge[label = $m'$](D)(E)
   \Edge[label = $n'$](E)(F)
   \tikzset{EdgeStyle/.append style = {bend left}}
   \Edge[label = $o'$](F)(D)
   \tikzset{EdgeStyle/.append style = {bend left}}
   \Edge[label = $p$](D)(F)  
\end{tikzpicture}
\caption{Example Input Ontologies with Object Properties}
\label{fig:input}
\end{figure*}

\begin{figure*}
\begin{equation*}
\left( \begin{array}{ccccccccc}
0 & b & c & a & a & a & a & a & a \\
d & 0 & f & a & a & a & a & a & a \\
g & h & 0 & a & a & a & a & a & a \\
g & h & i & 0 & a & a & a & a & a \\
g & h & i & j & 0 & a & a & a & a \\
g & h & 0 & a & a & 0 & a & a & a \\
g & h & 0 & a & a & a & 0 & a & a \\
g & h & 0 & a & a & a & a & 0 & a \\
g & h & 0 & a & a & a & a & a & 0 \end{array} \right)
\end{equation*}
\caption{Example an Unnormalized Pairwise Markov Chain}
\label{fig:upmc}
\end{figure*}

\subsection{Normalized Pairwise Markov Chain}

In order to take advantage of the convergance properties outlined earlier in this paper, the UPMC needs to have its probability transition matrix converted into a row stochastic form. 
This is done by normalizing the row sums of the matrix such that they sum to one.

\begin{defn}
\label{def:npmc}
a {\bf Normalized Pairwise Markov Chain (NPMC)} is defined as the Markov Chain generated from a UPMC by normalizing the row sums of the matrix such that they sum to one. Let $P$ be the $1$-step probability transition matrix for an UPMC. First, determine the sum of the current recipricol row sums.

$$ M_i = \sum \frac{1}{P_i} $$

\noindent Then, add the repricol row sums to each value in the matrix, storing each value in a temporary matrix $T$.

$$ T_{i,j} = -\frac{1}{P_{i,j}} + M_i $$

\noindent Normalize each value by dividing each value in $T$ by its row sum.

$$ P_{i,j} = \frac{T_{i,j}}{\sum T_i} $$
\end{defn}

\noindent The matrix that results from applying the transformation described in Definition~\ref{def:npmc} is \textit{row stochastic} (its rows sum to one).
When viewed as the $1$-step probability transition matrix for a Markov Chain, it is easy to see that the weights on the outgoing edges for each state sum to one.
After the transformation, a NPMC represents a Markov Chain where each pair of ontological terms has a certain probability of being related to other pairs in the chain in proportion to the edge confidences that were calculated earlier.

\subsection{Iterative Approach}

Once approach to finding the stationary distribution of the NPMC is to compute the limiting probabilistic state $\lim_{k \to \infty} \pi P^k$ in an iterative fashion. 
Let $\epsilon$ be some threshold.

$$ A = \pi^t : \pi^t P \pm \epsilon = \pi^{t-1} $$

\noindent The values for the initial similarity distribution $\pi^0$ are taken from the non-structural similarity scores for the ontological terms corresponding to each state.

\subsection{Steady-State Approach}

Another approach to finding the stationary distribution of the NPMC is to compute the limiting probabilistic state $\lim_{k \to \infty} \pi P^k$ directly.
This can be done by solving a left eigenvalue problem: 
$$ \pi = \pi P \Rightarrow \pi (P - I) = 0$$
where the eigenvalue is $1$. 
Simply solve for $\pi$ by computing the left nullspace of the $P - I$ matrix (appropriately sliced) and then normalizing $\pi$ so that $\vert\vert \pi \vert\vert = 1$.

\subsection{Refining Results}

The results generated from using either the iterative approach or the steady-state approach are only somewhat meaningful.
The ouput of both procedures produces a two-dimensional distribution of similarity scores between the two input ontologies.
Although this satisfies the definition of an alignment as presented earlier in this paper, a user is likely more interested in the set of similarities that yield the best correspondance between the two input ontologies.

Take the alignment distribution generated by either the iterative or steady-state approach and decompose it into an $m \times n$ matrix.
Consider this matrix to be representative of a weighted bipartite graph.
Now, finding the set of similarities that yield the best correspondance between the two ontologies is simply a matter of solving the maximum-weighted bipartite graph matching problem using the generated graph.
In order to accomplish this task, the Hungarian algorithm is used~\cite{kuhn:1955:hungarian}.

\subsection{Implementation Details}

Talk about the implementation here..

\section{Evaluation}
\label{sec:eval}

In order to evaluate the model presented in this paper, the bibliographic ontology benchmark provided by the Ontology Alignment Evaluation Initiative (OAEI) was used.
This benchmark test library consists of data sets that are built from the bibliographic reference ontology.
Each test includes a reference alignment in order to facilitate the calculation of precision and recall.

Our initial comparison is the alignment between the bibliographic ontology and an ontology that contains synonyms.

\section{Results}
\label{sec:results}

TODO

\section{Conclusions \& Future Work}
\label{sec:conclusions}


Some possible work includes, but is not limitted to, the following:

\begin{itemize}
\item Tweak the definition of edge confidence so that it includes information about the structural relationships between different object properties.
For example, object properties have parent-child relationships similar to ontological terms.
A new edge confidence function could take advantage of these relationships in order to consider a more structural approach to the alignment problem.
\item TODO
\end{itemize}

All of the source code for this project can be found at ...

%ACKNOWLEDGMENTS are optional
\section*{Acknowledgments}

We would like to acknowledge ...

%\bibliographystyle{abbrvnat}
\bibliographystyle{abbrv}
\bibliography{refs} 

\section*{Author Biographies} 
\vspace{8 pt}
\noindent \textbf{MICHAEL E. COTTERELL} is a Ph.D. student in Computer Science at the University of Georgia. 
He received his B.S. in Computer Science from UGA in May 2011. 
As an undergraduate, he served as the Vice President of the UGA Chapter of the Association for Computer Machinery (ACM) for two years. 
He also interned at the National Renewable Energy Lab, where he conducted research on formal ontologies for Energy Systems Integration and the Energy Informatics domain. 
His research interests include Simulation, Optimization, Semantic Web and Domain-Specific Languages with inter-disciplinary applications related to both Bioinformatics and Energy Informatics.
His email address is \href{mailto:mepcott@uga.edu}{mepcott@uga.edu}.
His curriculum vitae and list of publications are available at \url{http://michaelcotterell.com/}.

\vspace{8 pt}
\noindent \textbf{TERRANCE MEDINA} is a ... His email address is \href{mailto:medinat@uga.edu}{medinat@uga.edu}.




\end{document}


